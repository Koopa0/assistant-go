# Core æ ¸å¿ƒæ™ºèƒ½ç³»çµ±

é€™å€‹åŒ…å¯¦ç¾äº† Assistant çš„æ ¸å¿ƒæ™ºèƒ½ç³»çµ±ï¼ŒåŒ…æ‹¬ä»£ç†ç®¡ç†ã€ä¸Šä¸‹æ–‡å¼•æ“ã€è¨˜æ†¶ç³»çµ±å’Œå­¸ç¿’æ¡†æ¶ã€‚

## æ¶æ§‹æ¦‚è¿°

æ ¸å¿ƒç³»çµ±æ¡ç”¨åˆ†å±¤æ™ºèƒ½æ¶æ§‹ï¼Œå¯¦ç¾çœŸæ­£çš„èªçŸ¥è¨ˆç®—ï¼š

```
core/
â”œâ”€â”€ agents/           # æ™ºèƒ½ä»£ç†ç³»çµ±
â”‚   â”œâ”€â”€ base.go       # ä»£ç†åŸºç¤æ¡†æ¶
â”‚   â”œâ”€â”€ manager.go    # ä»£ç†ç®¡ç†å™¨
â”‚   â””â”€â”€ development.go # é–‹ç™¼å°ˆå®¶ä»£ç†
â”œâ”€â”€ context/          # ä¸Šä¸‹æ–‡å¼•æ“
â”‚   â”œâ”€â”€ engine.go     # ä¸Šä¸‹æ–‡åˆ†æå¼•æ“
â”‚   â”œâ”€â”€ workspace.go  # å·¥ä½œç©ºé–“ä¸Šä¸‹æ–‡
â”‚   â”œâ”€â”€ semantic.go   # èªç¾©ä¸Šä¸‹æ–‡
â”‚   â””â”€â”€ temporal.go   # æ™‚é–“ä¸Šä¸‹æ–‡
â”œâ”€â”€ memory/           # å¤šå±¤è¨˜æ†¶ç³»çµ±
â”‚   â”œâ”€â”€ types.go      # è¨˜æ†¶é¡å‹å®šç¾©
â”‚   â”œâ”€â”€ working.go    # å·¥ä½œè¨˜æ†¶
â”‚   â”œâ”€â”€ episodic.go   # æƒ…ç¯€è¨˜æ†¶
â”‚   â”œâ”€â”€ semantic.go   # èªç¾©è¨˜æ†¶
â”‚   â””â”€â”€ procedural.go # ç¨‹åºè¨˜æ†¶
â”œâ”€â”€ intelligence/     # æ™ºèƒ½æ¨ç†
â””â”€â”€ learning/         # å­¸ç¿’æ¡†æ¶
```

## è¨­è¨ˆå“²å­¸

### ğŸ§  èªçŸ¥æ¶æ§‹
åŸºæ–¼èªçŸ¥ç§‘å­¸è¨­è¨ˆçš„å¤šå±¤æ™ºèƒ½ç³»çµ±ï¼š
- **å·¥ä½œè¨˜æ†¶**: çŸ­æœŸæ´»å‹•è¨Šæ¯çš„å¿«é€Ÿå­˜å–
- **æƒ…ç¯€è¨˜æ†¶**: é–‹ç™¼ç¶“æ­·å’Œäº’å‹•æ­·å²çš„å­˜å„²
- **èªç¾©è¨˜æ†¶**: çŸ¥è­˜å’Œæ¦‚å¿µçš„çµæ§‹åŒ–çµ„ç¹”
- **ç¨‹åºè¨˜æ†¶**: æŠ€èƒ½å’Œæµç¨‹çš„è‡ªå‹•åŒ–åŸ·è¡Œ

### ğŸ¤ ä»£ç†å”ä½œ
å°ˆæ¥­åŒ–ä»£ç†çš„æ™ºèƒ½å”ä½œç¶²çµ¡ï¼š
- **é–‹ç™¼ä»£ç†**: ä»£ç¢¼ç†è§£ã€é‡æ§‹å»ºè­°ã€æœ€ä½³å¯¦è¸
- **è³‡æ–™åº«ä»£ç†**: æŸ¥è©¢å„ªåŒ–ã€æ¶æ§‹åˆ†æã€æ€§èƒ½èª¿å„ª
- **åŸºç¤è¨­æ–½ä»£ç†**: éƒ¨ç½²ç®¡ç†ã€ç›£æ§å‘Šè­¦ã€è³‡æºå„ªåŒ–
- **ç ”ç©¶ä»£ç†**: è¨Šæ¯ç¶œåˆã€çŸ¥è­˜æå–ã€æŠ€è¡“èª¿ç ”

### ğŸ“Š ä¸Šä¸‹æ–‡æ„ŸçŸ¥
å…¨æ–¹ä½çš„ç’°å¢ƒç†è§£å’Œé©æ‡‰ï¼š
- **å·¥ä½œç©ºé–“æ„ŸçŸ¥**: é …ç›®çµæ§‹ã€æ–‡ä»¶ç‹€æ…‹ã€é…ç½®è¨Šæ¯
- **èªç¾©ç†è§£**: ä»£ç¢¼æ„åœ–ã€æ¥­å‹™é‚è¼¯ã€æ¶æ§‹æ¨¡å¼
- **æ™‚é–“æ„ŸçŸ¥**: é–‹ç™¼æ­·å²ã€è®ŠåŒ–è¶¨å‹¢ã€ç¯€å¥æ¨¡å¼
- **ç¤¾äº¤æ„ŸçŸ¥**: åœ˜éšŠå”ä½œã€è§’è‰²æ¬Šé™ã€æºé€šåå¥½

## æ™ºèƒ½ä»£ç†ç³»çµ±

### ä»£ç†åŸºç¤æ¶æ§‹

```go
// Agent å®šç¾©æ™ºèƒ½ä»£ç†çš„æ ¸å¿ƒæ¥å£
type Agent interface {
    // ä»£ç†è­˜åˆ¥
    ID() string
    Name() string
    Domain() AgentDomain
    
    // æ ¸å¿ƒèƒ½åŠ›
    Capabilities() []Capability
    ProcessIntent(ctx context.Context, intent *Intent) (*AgentResponse, error)
    
    // å”ä½œèƒ½åŠ›
    EstimateConfidence(intent *Intent) float64
    RequestCollaboration(ctx context.Context, collaborators []Agent, intent *Intent) (*CollaborativeResponse, error)
    
    // å­¸ç¿’å’Œé©æ‡‰
    LearnFromInteraction(interaction *Interaction) error
    AdaptToFeedback(feedback *Feedback) error
    
    // ç”Ÿå‘½é€±æœŸ
    Initialize(ctx context.Context, config *AgentConfig) error
    Shutdown(ctx context.Context) error
}

// BaseAgent æä¾›ä»£ç†çš„åŸºç¤å¯¦ç¾
type BaseAgent struct {
    id           string
    name         string
    domain       AgentDomain
    capabilities []Capability
    
    // æ™ºèƒ½çµ„ä»¶
    reasoningEngine  *ReasoningEngine
    memorySystem     *MemorySystem
    learningEngine   *LearningEngine
    
    // å”ä½œçµ„ä»¶
    collaborationManager *CollaborationManager
    confidenceEstimator  *ConfidenceEstimator
    
    // ç›£æ§çµ„ä»¶
    metrics *AgentMetrics
    logger  *slog.Logger
}

func (ba *BaseAgent) ProcessIntent(ctx context.Context, intent *Intent) (*AgentResponse, error) {
    // 1. åˆ†ææ„åœ–ç›¸é—œæ€§
    relevance := ba.analyzeRelevance(intent)
    if relevance < 0.1 {
        return nil, ErrIntentNotRelevant
    }
    
    // 2. ä¼°è¨ˆè™•ç†ä¿¡å¿ƒåº¦
    confidence := ba.confidenceEstimator.Estimate(intent, ba.capabilities)
    if confidence < ba.config.MinConfidence {
        // å°‹æ±‚å”ä½œ
        return ba.requestCollaboration(ctx, intent)
    }
    
    // 3. åŸ·è¡Œæ¨ç†è™•ç†
    result, err := ba.reasoningEngine.Process(ctx, intent)
    if err != nil {
        return nil, fmt.Errorf("reasoning failed: %w", err)
    }
    
    // 4. ç”Ÿæˆå›æ‡‰
    response := &AgentResponse{
        AgentID:    ba.id,
        Content:    result.Content,
        Confidence: confidence,
        Reasoning:  result.Reasoning,
        Actions:    result.Actions,
        Metadata:   result.Metadata,
    }
    
    // 5. è¨˜éŒ„å­¸ç¿’æ•¸æ“š
    ba.recordInteraction(intent, response)
    
    return response, nil
}
```

### é–‹ç™¼å°ˆå®¶ä»£ç†

```go
// DevelopmentAgent å°ˆé–€è™•ç†é–‹ç™¼ç›¸é—œä»»å‹™çš„æ™ºèƒ½ä»£ç†
type DevelopmentAgent struct {
    *BaseAgent
    
    // å°ˆæ¥­å·¥å…·
    codeAnalyzer     *code.Analyzer
    patternDetector  *patterns.Detector
    qualityAssessor  *quality.Assessor
    refactoringEngine *refactoring.Engine
    
    // çŸ¥è­˜åº«
    bestPractices    *knowledge.BestPractices
    designPatterns   *knowledge.DesignPatterns
    languageSpecs    *knowledge.LanguageSpecs
}

func NewDevelopmentAgent(config *AgentConfig, logger *slog.Logger) (*DevelopmentAgent, error) {
    base, err := NewBaseAgent("dev-agent", "Development Expert", DomainDevelopment, config, logger)
    if err != nil {
        return nil, fmt.Errorf("failed to create base agent: %w", err)
    }
    
    agent := &DevelopmentAgent{
        BaseAgent:    base,
        codeAnalyzer: code.NewAnalyzer(),
        patternDetector: patterns.NewDetector(),
        qualityAssessor: quality.NewAssessor(),
        refactoringEngine: refactoring.NewEngine(),
    }
    
    // å®šç¾©é–‹ç™¼ä»£ç†çš„èƒ½åŠ›
    agent.capabilities = []Capability{
        {Name: "code_analysis", Level: ExpertLevel, Weight: 1.0},
        {Name: "refactoring", Level: ExpertLevel, Weight: 0.9},
        {Name: "best_practices", Level: ExpertLevel, Weight: 0.8},
        {Name: "pattern_detection", Level: AdvancedLevel, Weight: 0.7},
        {Name: "quality_assessment", Level: ExpertLevel, Weight: 0.85},
    }
    
    return agent, nil
}

func (da *DevelopmentAgent) ProcessIntent(ctx context.Context, intent *Intent) (*AgentResponse, error) {
    // æª¢æŸ¥æ˜¯å¦ç‚ºé–‹ç™¼ç›¸é—œæ„åœ–
    if !da.isRelevantIntent(intent) {
        return nil, ErrIntentNotRelevant
    }
    
    switch intent.Type {
    case IntentCodeAnalysis:
        return da.analyzeCode(ctx, intent)
    case IntentRefactoring:
        return da.suggestRefactoring(ctx, intent)
    case IntentCodeReview:
        return da.reviewCode(ctx, intent)
    case IntentArchitectureAdvice:
        return da.provideArchitectureAdvice(ctx, intent)
    default:
        // ä½¿ç”¨åŸºç¤è™•ç†é‚è¼¯
        return da.BaseAgent.ProcessIntent(ctx, intent)
    }
}

func (da *DevelopmentAgent) analyzeCode(ctx context.Context, intent *Intent) (*AgentResponse, error) {
    codeInput := intent.Payload.(*CodeAnalysisInput)
    
    // 1. è§£æä»£ç¢¼çµæ§‹
    ast, err := da.codeAnalyzer.ParseCode(codeInput.Language, codeInput.Source)
    if err != nil {
        return nil, fmt.Errorf("code parsing failed: %w", err)
    }
    
    // 2. åˆ†æä»£ç¢¼è³ªé‡
    quality := da.qualityAssessor.Assess(ast)
    
    // 3. æª¢æ¸¬è¨­è¨ˆæ¨¡å¼
    patterns := da.patternDetector.DetectPatterns(ast)
    
    // 4. è­˜åˆ¥æ”¹é€²æ©Ÿæœƒ
    improvements := da.identifyImprovements(ast, quality, patterns)
    
    // 5. ç”Ÿæˆåˆ†æå ±å‘Š
    analysis := &CodeAnalysisResult{
        Quality:      quality,
        Patterns:     patterns,
        Improvements: improvements,
        Metrics:      da.calculateMetrics(ast),
        Suggestions:  da.generateSuggestions(improvements),
    }
    
    return &AgentResponse{
        AgentID:    da.ID(),
        Content:    da.formatAnalysisReport(analysis),
        Confidence: da.calculateConfidence(analysis),
        Actions:    da.suggestActions(analysis),
        Metadata: map[string]interface{}{
            "analysis_type": "code_analysis",
            "language":      codeInput.Language,
            "complexity":    quality.CyclomaticComplexity,
            "maintainability": quality.MaintainabilityIndex,
        },
    }, nil
}
```

## ä¸Šä¸‹æ–‡å¼•æ“

### å¤šç¶­ä¸Šä¸‹æ–‡åˆ†æ

```go
// ContextEngine ç®¡ç†å’Œåˆ†æå¤šç¶­é–‹ç™¼ä¸Šä¸‹æ–‡
type ContextEngine struct {
    // åˆ†æå™¨çµ„ä»¶
    workspaceAnalyzer *workspace.Analyzer
    semanticAnalyzer  *semantic.Analyzer
    temporalAnalyzer  *temporal.Analyzer
    socialAnalyzer    *social.Analyzer
    
    // ä¸Šä¸‹æ–‡å­˜å„²
    contextStore *ContextStore
    contextCache *cache.LRU[string, *Context]
    
    // å­¸ç¿’çµ„ä»¶
    patternLearner *PatternLearner
    
    // ç›£æ§çµ„ä»¶
    metrics *ContextMetrics
    logger  *slog.Logger
}

func (ce *ContextEngine) AnalyzeContext(trigger *ContextTrigger) (*Context, error) {
    // 1. åŸºç¤ä¸Šä¸‹æ–‡æ”¶é›†
    baseContext := ce.collectBaseContext(trigger)
    
    // 2. å·¥ä½œç©ºé–“åˆ†æ
    workspaceCtx, err := ce.workspaceAnalyzer.Analyze(trigger.WorkspacePath)
    if err != nil {
        return nil, fmt.Errorf("workspace analysis failed: %w", err)
    }
    
    // 3. èªç¾©åˆ†æ
    semanticCtx := ce.semanticAnalyzer.AnalyzeSemantics(workspaceCtx)
    
    // 4. æ™‚é–“åˆ†æ
    temporalCtx := ce.temporalAnalyzer.AnalyzeTemporal(trigger.Timestamp, workspaceCtx)
    
    // 5. ç¤¾äº¤åˆ†æ
    socialCtx := ce.socialAnalyzer.AnalyzeSocial(trigger.UserID, workspaceCtx)
    
    // 6. ç¶œåˆä¸Šä¸‹æ–‡
    context := &Context{
        ID:        generateContextID(),
        Timestamp: trigger.Timestamp,
        UserID:    trigger.UserID,
        
        Base:      baseContext,
        Workspace: workspaceCtx,
        Semantic:  semanticCtx,
        Temporal:  temporalCtx,
        Social:    socialCtx,
        
        Confidence: ce.calculateContextConfidence(workspaceCtx, semanticCtx, temporalCtx),
        Metadata:   make(map[string]interface{}),
    }
    
    // 7. æ¨¡å¼å­¸ç¿’
    ce.patternLearner.LearnFromContext(context)
    
    // 8. ç·©å­˜ä¸Šä¸‹æ–‡
    ce.contextCache.Set(context.ID, context)
    
    return context, nil
}
```

### èªç¾©ä¸Šä¸‹æ–‡åˆ†æ

```go
// SemanticAnalyzer åˆ†æä»£ç¢¼å’Œé …ç›®çš„èªç¾©ä¸Šä¸‹æ–‡
type SemanticAnalyzer struct {
    // èªè¨€åˆ†æå™¨
    languageAnalyzers map[string]LanguageAnalyzer
    
    // èªç¾©æ¨¡å‹
    codeEmbeddings    *embeddings.CodeEmbeddings
    conceptGraph      *graph.ConceptGraph
    
    // æ¨¡å¼è­˜åˆ¥
    architectureDetector *arch.Detector
    frameworkDetector    *framework.Detector
}

func (sa *SemanticAnalyzer) AnalyzeSemantics(workspace *WorkspaceContext) *SemanticContext {
    semanticCtx := &SemanticContext{
        Concepts:      make([]Concept, 0),
        Relationships: make([]Relationship, 0),
        Patterns:      make([]SemanticPattern, 0),
        Intent:        make([]IntentClue, 0),
    }
    
    // 1. åˆ†æä¸»è¦ç·¨ç¨‹èªè¨€
    primaryLang := sa.detectPrimaryLanguage(workspace.Files)
    if analyzer, exists := sa.languageAnalyzers[primaryLang]; exists {
        langSemantics := analyzer.AnalyzeSemantics(workspace.Files)
        semanticCtx.Language = langSemantics
    }
    
    // 2. æª¢æ¸¬æ¶æ§‹æ¨¡å¼
    archPatterns := sa.architectureDetector.DetectPatterns(workspace)
    semanticCtx.Architecture = &ArchitectureSemantics{
        Patterns:    archPatterns,
        Style:       sa.determineArchitecturalStyle(archPatterns),
        Complexity:  sa.calculateArchitecturalComplexity(archPatterns),
    }
    
    // 3. è­˜åˆ¥æ¡†æ¶å’Œåº«
    frameworks := sa.frameworkDetector.DetectFrameworks(workspace)
    semanticCtx.Frameworks = frameworks
    
    // 4. æ§‹å»ºæ¦‚å¿µåœ–
    concepts := sa.extractConcepts(workspace, frameworks)
    semanticCtx.Concepts = concepts
    
    // 5. åˆ†æé—œä¿‚ç¶²çµ¡
    relationships := sa.analyzeRelationships(concepts, workspace)
    semanticCtx.Relationships = relationships
    
    // 6. æ¨æ–·æ„åœ–ç·šç´¢
    intentClues := sa.inferIntentClues(workspace, concepts, relationships)
    semanticCtx.Intent = intentClues
    
    return semanticCtx
}
```

## è¨˜æ†¶ç³»çµ±

### å¤šå±¤è¨˜æ†¶æ¶æ§‹

```go
// MemorySystem å¯¦ç¾å¤šå±¤èªçŸ¥è¨˜æ†¶æ¶æ§‹
type MemorySystem struct {
    // è¨˜æ†¶å±¤
    workingMemory   *WorkingMemory
    episodicMemory  *EpisodicMemory
    semanticMemory  *SemanticMemory
    proceduralMemory *ProceduralMemory
    
    // å”èª¿çµ„ä»¶
    memoryCoordinator *MemoryCoordinator
    consolidationEngine *ConsolidationEngine
    
    // å­˜å„²å¾Œç«¯
    storage MemoryStorage
    
    // é…ç½®
    config *MemoryConfig
    logger *slog.Logger
}

// WorkingMemory å·¥ä½œè¨˜æ†¶ï¼šçŸ­æœŸã€é«˜é€Ÿçš„ä»»å‹™ç›¸é—œè¨Šæ¯
type WorkingMemory struct {
    // ç•¶å‰ä»»å‹™ä¸Šä¸‹æ–‡
    currentTask     *Task
    activeVariables map[string]interface{}
    
    // æ³¨æ„åŠ›ç„¦é»
    attentionFocus  []AttentionItem
    
    // çŸ­æœŸç·©å­˜
    shortTermCache  *cache.LRU[string, MemoryItem]
    capacity        int
    
    // è¡°æ¸›æ©Ÿåˆ¶
    decayFunction   DecayFunction
    lastAccess      map[string]time.Time
}

func (wm *WorkingMemory) Store(key string, value interface{}, importance float64) error {
    // æª¢æŸ¥å®¹é‡é™åˆ¶
    if wm.shortTermCache.Len() >= wm.capacity {
        // åŸºæ–¼é‡è¦æ€§å’Œæ™‚é–“è¡°æ¸›ç§»é™¤é …ç›®
        wm.evictLeastImportant()
    }
    
    item := &MemoryItem{
        Key:         key,
        Value:       value,
        Importance:  importance,
        Timestamp:   time.Now(),
        AccessCount: 1,
    }
    
    wm.shortTermCache.Set(key, item)
    wm.lastAccess[key] = time.Now()
    
    // æ›´æ–°æ³¨æ„åŠ›ç„¦é»
    wm.updateAttentionFocus(key, importance)
    
    return nil
}

// EpisodicMemory æƒ…ç¯€è¨˜æ†¶ï¼šé–‹ç™¼ç¶“æ­·å’Œäº’å‹•æ­·å²
type EpisodicMemory struct {
    storage    EpisodicStorage
    indexer    *EpisodicIndexer
    retriever  *EpisodicRetriever
    
    // æ™‚é–“çµ„ç¹”
    timelineIndex  *TimelineIndex
    
    // æƒ…å¢ƒçµ„ç¹”
    contextIndex   *ContextIndex
    
    // æƒ…æ„Ÿæ¨™è¨˜
    emotionalTagger *EmotionalTagger
}

func (em *EpisodicMemory) StoreEpisode(episode *Episode) error {
    // 1. æƒ…æ„Ÿæ¨™è¨˜
    emotion := em.emotionalTagger.TagEmotion(episode)
    episode.Emotion = emotion
    
    // 2. æ™‚é–“ç´¢å¼•
    err := em.timelineIndex.IndexByTime(episode)
    if err != nil {
        return fmt.Errorf("timeline indexing failed: %w", err)
    }
    
    // 3. æƒ…å¢ƒç´¢å¼•
    err = em.contextIndex.IndexByContext(episode)
    if err != nil {
        return fmt.Errorf("context indexing failed: %w", err)
    }
    
    // 4. å­˜å„²æƒ…ç¯€
    err = em.storage.Store(episode)
    if err != nil {
        return fmt.Errorf("episode storage failed: %w", err)
    }
    
    return nil
}

func (em *EpisodicMemory) RetrieveRelevantEpisodes(query *EpisodicQuery) ([]*Episode, error) {
    // 1. æ™‚é–“éæ¿¾
    timeFiltered := em.timelineIndex.FilterByTimeRange(query.TimeRange)
    
    // 2. æƒ…å¢ƒç›¸ä¼¼æ€§
    contextFiltered := em.contextIndex.FindSimilarContexts(query.Context, timeFiltered)
    
    // 3. å…§å®¹ç›¸é—œæ€§
    contentFiltered := em.retriever.FilterByContent(query.Content, contextFiltered)
    
    // 4. é‡è¦æ€§æ’åº
    ranked := em.rankByImportance(contentFiltered, query)
    
    // 5. é™åˆ¶è¿”å›æ•¸é‡
    if len(ranked) > query.Limit {
        ranked = ranked[:query.Limit]
    }
    
    return ranked, nil
}
```

### è¨˜æ†¶æ•´åˆæ©Ÿåˆ¶

```go
// ConsolidationEngine è² è²¬è¨˜æ†¶çš„æ•´åˆå’Œè½‰ç§»
type ConsolidationEngine struct {
    consolidationScheduler *Scheduler
    patternExtractor      *PatternExtractor
    knowledgeGenerator    *KnowledgeGenerator
    
    // æ•´åˆç­–ç•¥
    strategies map[MemoryType]ConsolidationStrategy
}

func (ce *ConsolidationEngine) ConsolidateMemories(ctx context.Context) error {
    // 1. å·¥ä½œè¨˜æ†¶åˆ°æƒ…ç¯€è¨˜æ†¶çš„è½‰ç§»
    err := ce.consolidateWorkingToEpisodic(ctx)
    if err != nil {
        return fmt.Errorf("working to episodic consolidation failed: %w", err)
    }
    
    // 2. æƒ…ç¯€è¨˜æ†¶åˆ°èªç¾©è¨˜æ†¶çš„æŠ½è±¡
    err = ce.consolidateEpisodicToSemantic(ctx)
    if err != nil {
        return fmt.Errorf("episodic to semantic consolidation failed: %w", err)
    }
    
    // 3. ç¨‹åºè¨˜æ†¶çš„å¼·åŒ–
    err = ce.consolidateProcedural(ctx)
    if err != nil {
        return fmt.Errorf("procedural consolidation failed: %w", err)
    }
    
    return nil
}

func (ce *ConsolidationEngine) consolidateEpisodicToSemantic(ctx context.Context) error {
    // ç²å–æœ€è¿‘çš„æƒ…ç¯€è¨˜æ†¶
    recentEpisodes, err := ce.getRecentEpisodes(time.Now().Add(-24*time.Hour))
    if err != nil {
        return fmt.Errorf("failed to get recent episodes: %w", err)
    }
    
    // æå–æ¨¡å¼
    patterns := ce.patternExtractor.ExtractPatterns(recentEpisodes)
    
    // ç”ŸæˆçŸ¥è­˜
    for _, pattern := range patterns {
        if pattern.Confidence > 0.8 {
            knowledge := ce.knowledgeGenerator.GenerateKnowledge(pattern)
            err := ce.semanticMemory.StoreKnowledge(knowledge)
            if err != nil {
                ce.logger.Warn("Failed to store consolidated knowledge", 
                    slog.String("pattern", pattern.ID),
                    slog.Any("error", err))
            }
        }
    }
    
    return nil
}
```

## å­¸ç¿’æ¡†æ¶

### è‡ªé©æ‡‰å­¸ç¿’å¼•æ“

```go
// LearningEngine å¯¦ç¾è‡ªé©æ‡‰å­¸ç¿’å’ŒçŸ¥è­˜æ›´æ–°
type LearningEngine struct {
    // å­¸ç¿’æ¨¡çµ„
    patternLearner     *PatternLearner
    preferenceLearner  *PreferenceLearner
    skillAssessor      *SkillAssessor
    adaptationEngine   *AdaptationEngine
    
    // å­¸ç¿’å­˜å„²
    learningStorage    LearningStorage
    
    // å­¸ç¿’ç­–ç•¥
    strategies         map[LearningType]LearningStrategy
    
    // è©•ä¼°æŒ‡æ¨™
    metrics           *LearningMetrics
    logger            *slog.Logger
}

func (le *LearningEngine) LearnFromInteraction(interaction *Interaction) error {
    // 1. æ¨¡å¼å­¸ç¿’
    patterns := le.patternLearner.ExtractPatterns(interaction)
    for _, pattern := range patterns {
        err := le.learningStorage.StorePattern(pattern)
        if err != nil {
            le.logger.Warn("Failed to store learned pattern", slog.Any("error", err))
        }
    }
    
    // 2. åå¥½å­¸ç¿’
    preferences := le.preferenceLearner.InferPreferences(interaction)
    err := le.learningStorage.UpdatePreferences(interaction.UserID, preferences)
    if err != nil {
        le.logger.Warn("Failed to update user preferences", slog.Any("error", err))
    }
    
    // 3. æŠ€èƒ½è©•ä¼°
    skillUpdate := le.skillAssessor.AssessSkillChange(interaction)
    if skillUpdate != nil {
        err := le.learningStorage.UpdateSkillAssessment(interaction.UserID, skillUpdate)
        if err != nil {
            le.logger.Warn("Failed to update skill assessment", slog.Any("error", err))
        }
    }
    
    // 4. ç³»çµ±é©æ‡‰
    adaptations := le.adaptationEngine.GenerateAdaptations(interaction, patterns, preferences)
    for _, adaptation := range adaptations {
        err := le.applyAdaptation(adaptation)
        if err != nil {
            le.logger.Warn("Failed to apply adaptation", slog.Any("error", err))
        }
    }
    
    // 5. è¨˜éŒ„å­¸ç¿’æŒ‡æ¨™
    le.metrics.RecordLearningEvent(interaction, len(patterns), len(preferences), len(adaptations))
    
    return nil
}
```

### æ¨¡å¼å­¸ç¿’å™¨

```go
// PatternLearner å¾ç”¨æˆ¶äº’å‹•ä¸­å­¸ç¿’è¡Œç‚ºæ¨¡å¼
type PatternLearner struct {
    // æ¨¡å¼æª¢æ¸¬
    sequenceDetector  *SequenceDetector
    frequencyAnalyzer *FrequencyAnalyzer
    contextClusterer  *ContextClusterer
    
    // æ¨¡å¼å­˜å„²
    patternStore      PatternStore
    
    // å­¸ç¿’åƒæ•¸
    minSupport        float64
    minConfidence     float64
    windowSize        int
}

func (pl *PatternLearner) ExtractPatterns(interaction *Interaction) []*Pattern {
    patterns := make([]*Pattern, 0)
    
    // 1. åºåˆ—æ¨¡å¼æª¢æ¸¬
    sequences := pl.sequenceDetector.DetectSequences(interaction.History)
    for _, seq := range sequences {
        if seq.Support >= pl.minSupport && seq.Confidence >= pl.minConfidence {
            pattern := &Pattern{
                Type:       PatternTypeSequence,
                Data:       seq,
                Support:    seq.Support,
                Confidence: seq.Confidence,
                Context:    interaction.Context,
                Timestamp:  time.Now(),
            }
            patterns = append(patterns, pattern)
        }
    }
    
    // 2. é »ç‡æ¨¡å¼åˆ†æ
    frequencies := pl.frequencyAnalyzer.AnalyzeFrequencies(interaction.History)
    for _, freq := range frequencies {
        if freq.Significance > 0.7 {
            pattern := &Pattern{
                Type:       PatternTypeFrequency,
                Data:       freq,
                Support:    freq.RelativeFrequency,
                Confidence: freq.Significance,
                Context:    interaction.Context,
                Timestamp:  time.Now(),
            }
            patterns = append(patterns, pattern)
        }
    }
    
    // 3. ä¸Šä¸‹æ–‡èšé¡æ¨¡å¼
    clusters := pl.contextClusterer.ClusterContexts(interaction.History)
    for _, cluster := range clusters {
        if cluster.Cohesion > 0.8 {
            pattern := &Pattern{
                Type:       PatternTypeContext,
                Data:       cluster,
                Support:    float64(cluster.Size) / float64(len(interaction.History)),
                Confidence: cluster.Cohesion,
                Context:    interaction.Context,
                Timestamp:  time.Now(),
            }
            patterns = append(patterns, pattern)
        }
    }
    
    return patterns
}
```

## é…ç½®å’Œéƒ¨ç½²

### æ ¸å¿ƒç³»çµ±é…ç½®

```yaml
core:
  # ä»£ç†ç³»çµ±é…ç½®
  agents:
    max_concurrent: 5
    timeout: 30s
    collaboration_enabled: true
    confidence_threshold: 0.7
    
    # å€‹åˆ¥ä»£ç†é…ç½®
    development:
      enabled: true
      max_memory: 512MB
      specializations:
        - "go"
        - "python"
        - "javascript"
    
    database:
      enabled: true
      max_memory: 256MB
      supported_databases:
        - "postgresql"
        - "mysql"
        - "mongodb"
  
  # ä¸Šä¸‹æ–‡å¼•æ“é…ç½®
  context:
    analysis_depth: "deep"
    cache_size: 1000
    cache_ttl: 1h
    semantic_analysis: true
    temporal_analysis: true
    
  # è¨˜æ†¶ç³»çµ±é…ç½®
  memory:
    working_memory:
      capacity: 100
      decay_rate: 0.1
      consolidation_interval: 10m
    
    episodic_memory:
      retention_days: 90
      max_episodes: 10000
      compression_enabled: true
    
    semantic_memory:
      concept_graph_size: 5000
      relationship_depth: 3
      update_frequency: 1h
    
    procedural_memory:
      skill_tracking: true
      automation_threshold: 0.9
      reinforcement_cycles: 5
  
  # å­¸ç¿’æ¡†æ¶é…ç½®
  learning:
    enabled: true
    pattern_detection:
      min_support: 0.3
      min_confidence: 0.7
      window_size: 50
    
    preference_learning:
      adaptation_rate: 0.1
      feedback_weight: 0.8
      decay_factor: 0.95
    
    skill_assessment:
      assessment_frequency: 24h
      skill_categories:
        - "coding"
        - "debugging"
        - "architecture"
        - "testing"
```

## æ€§èƒ½ç›£æ§

### æ™ºèƒ½ç³»çµ±æŒ‡æ¨™

```go
// CoreMetrics æ ¸å¿ƒç³»çµ±æ€§èƒ½æŒ‡æ¨™
type CoreMetrics struct {
    // ä»£ç†æŒ‡æ¨™
    AgentResponseTimes    map[string]time.Duration
    AgentConfidenceScores map[string]float64
    AgentCollaborations   map[string]int64
    
    // ä¸Šä¸‹æ–‡æŒ‡æ¨™
    ContextAnalysisTime   time.Duration
    ContextAccuracy       float64
    ContextCacheHitRate   float64
    
    // è¨˜æ†¶æŒ‡æ¨™
    MemoryUtilization     map[MemoryType]float64
    MemoryConsolidationRate float64
    MemoryRetrievalTime   time.Duration
    
    // å­¸ç¿’æŒ‡æ¨™
    PatternsLearned       int64
    PreferencesUpdated    int64
    AdaptationsApplied    int64
    LearningAccuracy      float64
}

func (cm *CoreMetrics) RecordAgentInteraction(agentID string, responseTime time.Duration, confidence float64) {
    cm.AgentResponseTimes[agentID] = responseTime
    cm.AgentConfidenceScores[agentID] = confidence
    
    // è¨ˆç®—é‹è¡Œå¹³å‡å€¼
    cm.updateRunningAverages(agentID, responseTime, confidence)
}

func (cm *CoreMetrics) GenerateHealthReport() *HealthReport {
    report := &HealthReport{
        Timestamp: time.Now(),
        Overall:   HealthStatusHealthy,
        Components: make(map[string]ComponentHealth),
    }
    
    // ä»£ç†å¥åº·æª¢æŸ¥
    agentHealth := cm.assessAgentHealth()
    report.Components["agents"] = agentHealth
    
    // è¨˜æ†¶ç³»çµ±å¥åº·æª¢æŸ¥
    memoryHealth := cm.assessMemoryHealth()
    report.Components["memory"] = memoryHealth
    
    // å­¸ç¿’ç³»çµ±å¥åº·æª¢æŸ¥
    learningHealth := cm.assessLearningHealth()
    report.Components["learning"] = learningHealth
    
    // ç¢ºå®šæ•´é«”å¥åº·ç‹€æ…‹
    report.Overall = cm.determineOverallHealth(agentHealth, memoryHealth, learningHealth)
    
    return report
}
```

## æœ€ä½³å¯¦è¸

### 1. ä»£ç†è¨­è¨ˆåŸå‰‡

- **å°ˆæ¥­åŒ–**: æ¯å€‹ä»£ç†å°ˆæ³¨ç‰¹å®šé ˜åŸŸçš„å°ˆæ¥­çŸ¥è­˜
- **å”ä½œæ€§**: è¨­è¨ˆä»£ç†é–“çš„æœ‰æ•ˆå”ä½œæ©Ÿåˆ¶
- **å­¸ç¿’æ€§**: å¾æ¯æ¬¡äº’å‹•ä¸­å­¸ç¿’å’Œæ”¹é€²
- **å¯è§£é‡‹æ€§**: æä¾›æ±ºç­–éç¨‹çš„é€æ˜è§£é‡‹

### 2. è¨˜æ†¶ç®¡ç†ç­–ç•¥

- **åˆ†å±¤å­˜å„²**: æ ¹æ“šè¨ªå•é »ç‡å’Œé‡è¦æ€§åˆ†å±¤å­˜å„²
- **å®šæœŸæ•´åˆ**: å¯¦æ–½è¨˜æ†¶æ•´åˆæ©Ÿåˆ¶é¿å…ç¢ç‰‡åŒ–
- **æ™ºèƒ½éºå¿˜**: ä¸»å‹•éºå¿˜éæ™‚æˆ–ç„¡é—œè¨Šæ¯
- **éš±ç§ä¿è­·**: ç¢ºä¿å€‹äººè¨˜æ†¶æ•¸æ“šçš„å®‰å…¨æ€§

### 3. å­¸ç¿’å„ªåŒ–

- **å¢é‡å­¸ç¿’**: æ¡ç”¨å¢é‡å­¸ç¿’é¿å…ç½é›£æ€§éºå¿˜
- **å¤šæ¨¡æ…‹å­¸ç¿’**: çµåˆå¤šç¨®å­¸ç¿’ä¿¡è™Ÿæé«˜æº–ç¢ºæ€§
- **åé¥‹å¾ªç’°**: å»ºç«‹æœ‰æ•ˆçš„ç”¨æˆ¶åé¥‹æ©Ÿåˆ¶
- **æŒçºŒè©•ä¼°**: æŒçºŒè©•ä¼°å­¸ç¿’æ•ˆæœä¸¦èª¿æ•´ç­–ç•¥

---

*æ ¸å¿ƒæ™ºèƒ½ç³»çµ±æ˜¯ Assistant çš„å¤§è…¦ï¼Œé€šéèªçŸ¥æ¶æ§‹ã€æ™ºèƒ½ä»£ç†ã€å¤šå±¤è¨˜æ†¶å’Œè‡ªé©æ‡‰å­¸ç¿’ï¼Œå¯¦ç¾çœŸæ­£æ™ºèƒ½çš„é–‹ç™¼ä¼´ä¾¶é«”é©—ã€‚*