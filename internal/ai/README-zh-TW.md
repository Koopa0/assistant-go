# ğŸ§  AI Package - äººå·¥æ™ºæ…§æœå‹™æ•´åˆå¥—ä»¶

## ğŸ“‹ æ¦‚è¿°

AI å¥—ä»¶æ˜¯ Assistant ç³»çµ±çš„æ ¸å¿ƒæ™ºæ…§å¼•æ“ï¼Œè² è²¬æ•´åˆå¤šå€‹ AI æä¾›è€…ï¼ˆClaudeã€Geminiï¼‰ï¼Œæä¾›çµ±ä¸€çš„ä»‹é¢ä¾†è™•ç†è‡ªç„¶èªè¨€ç†è§£ã€ç¨‹å¼ç¢¼åˆ†æã€æ™ºæ…§å»ºè­°ç­‰åŠŸèƒ½ã€‚é€™å€‹å¥—ä»¶å¯¦ç¾äº†æä¾›è€…æŠ½è±¡ã€éŒ¯èª¤è™•ç†ã€ä¸²æµæ”¯æ´ã€åµŒå…¥å‘é‡ç­‰é—œéµåŠŸèƒ½ã€‚

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### æ ¸å¿ƒä»‹é¢

```go
// Service å®šç¾©äº† AI æœå‹™çš„çµ±ä¸€ä»‹é¢
type Service interface {
    // Chat ç™¼é€èŠå¤©è«‹æ±‚ä¸¦è¿”å›å®Œæ•´å›æ‡‰
    Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error)
    
    // ChatStream ç™¼é€èŠå¤©è«‹æ±‚ä¸¦è¿”å›ä¸²æµå›æ‡‰
    ChatStream(ctx context.Context, req ChatRequest) (<-chan StreamEvent, error)
    
    // GenerateEmbedding ç”Ÿæˆæ–‡å­—çš„å‘é‡åµŒå…¥
    GenerateEmbedding(ctx context.Context, text string) ([]float64, error)
    
    // GetCapabilities è¿”å›ç•¶å‰ AI æä¾›è€…çš„èƒ½åŠ›
    GetCapabilities() Capabilities
}
```

### æä¾›è€…æ¶æ§‹

```go
// æ”¯æ´çš„ AI æä¾›è€…
type Provider string

const (
    ProviderClaude Provider = "claude"  // Anthropic Claude
    ProviderGemini Provider = "gemini"  // Google Gemini
)

// æä¾›è€…ç®¡ç†
type ProviderManager struct {
    primary   Provider           // ä¸»è¦æä¾›è€…
    fallback  Provider           // å‚™ç”¨æä¾›è€…
    providers map[Provider]Service // æä¾›è€…å¯¦ä¾‹
    health    map[Provider]bool    // å¥åº·ç‹€æ…‹
}
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. å¤šæä¾›è€…æ”¯æ´ (Multi-Provider Support)

#### Claude æ•´åˆ
```go
// Claude å®¢æˆ¶ç«¯é…ç½®
type ClaudeClient struct {
    apiKey     string
    model      string           // claude-3-opus, claude-3-sonnet ç­‰
    maxTokens  int
    httpClient *http.Client
}

// ç‰¹æ€§æ”¯æ´
- è¶…é•·ä¸Šä¸‹æ–‡çª—å£ï¼ˆ200K tokensï¼‰
- å„ªç§€çš„ç¨‹å¼ç¢¼ç†è§£èƒ½åŠ›
- æ”¯æ´ç³»çµ±æç¤ºè©
- å³æ™‚ä¸²æµå›æ‡‰
```

#### Gemini æ•´åˆ
```go
// Gemini å®¢æˆ¶ç«¯é…ç½®
type GeminiClient struct {
    apiKey        string
    model         string         // gemini-pro, gemini-pro-vision ç­‰
    generationConfig *genai.GenerationConfig
    client        *genai.Client
}

// ç‰¹æ€§æ”¯æ´
- å¤šæ¨¡æ…‹æ”¯æ´ï¼ˆæ–‡å­—ã€åœ–ç‰‡ï¼‰
- å‡½æ•¸èª¿ç”¨èƒ½åŠ›
- é«˜é€Ÿå›æ‡‰
- æˆæœ¬æ•ˆç›Š
```

### 2. æ™ºæ…§è·¯ç”± (Intelligent Routing)

```go
// æ ¹æ“šè«‹æ±‚ç‰¹æ€§é¸æ“‡æœ€ä½³æä¾›è€…
func (m *ProviderManager) RouteRequest(req ChatRequest) Provider {
    // è€ƒæ…®å› ç´ ï¼š
    // 1. è«‹æ±‚è¤‡é›œåº¦
    if req.RequiresDeepAnalysis() {
        return ProviderClaude // Claude æ›´æ“…é•·æ·±åº¦åˆ†æ
    }
    
    // 2. æˆæœ¬è€ƒé‡
    if req.Priority == PriorityLow {
        return ProviderGemini // Gemini æˆæœ¬æ›´ä½
    }
    
    // 3. å¯ç”¨æ€§
    if !m.health[m.primary] {
        return m.fallback // ä½¿ç”¨å‚™ç”¨æä¾›è€…
    }
    
    // 4. ç‰¹æ®ŠåŠŸèƒ½éœ€æ±‚
    if req.RequiresVision() {
        return ProviderGemini // Gemini æ”¯æ´è¦–è¦º
    }
    
    return m.primary
}
```

### 3. ä¸²æµè™•ç† (Stream Processing)

```go
// çµ±ä¸€çš„ä¸²æµäº‹ä»¶
type StreamEvent struct {
    Type      StreamEventType
    Content   string          // æ–‡å­—å…§å®¹
    Delta     string          // å¢é‡å…§å®¹
    Error     error           // éŒ¯èª¤è³‡è¨Š
    Metadata  map[string]any  // å…ƒè³‡æ–™
    Timestamp time.Time
}

// ä¸²æµè™•ç†å™¨
type StreamProcessor struct {
    // ç·©è¡ç®¡ç†
    buffer     *bytes.Buffer
    bufferSize int
    
    // éŒ¯èª¤æ¢å¾©
    retryCount int
    maxRetries int
    
    // é€²åº¦è¿½è¹¤
    tokenCount int
    startTime  time.Time
}
```

### 4. åµŒå…¥å‘é‡æœå‹™ (Embeddings Service)

```go
// åµŒå…¥å‘é‡æœå‹™ä»‹é¢
type EmbeddingService interface {
    // ç”Ÿæˆå–®ä¸€æ–‡å­—çš„åµŒå…¥å‘é‡
    GenerateEmbedding(ctx context.Context, text string) ([]float64, error)
    
    // æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
    GenerateEmbeddings(ctx context.Context, texts []string) ([][]float64, error)
    
    // è¨ˆç®—å‘é‡ç›¸ä¼¼åº¦
    CosineSimilarity(a, b []float64) float64
}

// å¯¦ç¾ç´°ç¯€
type embeddingService struct {
    provider   string          // ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
    dimension  int            // å‘é‡ç¶­åº¦
    cache      *lru.Cache     // LRU å¿«å–
    batchSize  int            // æ‰¹æ¬¡å¤§å°
}
```

## ğŸ“Š é€²éšåŠŸèƒ½

### 1. æç¤ºè©å·¥ç¨‹ (Prompt Engineering)

```go
// æç¤ºè©æ¨¡æ¿ç³»çµ±
type PromptTemplate struct {
    Name        string
    Template    string
    Variables   []string
    Constraints []string
    Examples    []Example
}

// å‹•æ…‹æç¤ºè©ç”Ÿæˆ
func (s *promptService) GeneratePrompt(
    template PromptTemplate,
    context map[string]interface{},
) (string, error) {
    // 1. é©—è­‰å¿…è¦è®Šæ•¸
    if err := s.validateVariables(template, context); err != nil {
        return "", err
    }
    
    // 2. æ³¨å…¥ä¸Šä¸‹æ–‡
    prompt := s.injectContext(template.Template, context)
    
    // 3. æ·»åŠ ç´„æŸæ¢ä»¶
    prompt = s.applyConstraints(prompt, template.Constraints)
    
    // 4. æ·»åŠ ç¯„ä¾‹ï¼ˆFew-shot learningï¼‰
    if len(template.Examples) > 0 {
        prompt = s.addExamples(prompt, template.Examples)
    }
    
    return prompt, nil
}
```

### 2. Token ç®¡ç† (Token Management)

```go
// Token è¨ˆæ•¸å™¨
type TokenCounter interface {
    // è¨ˆç®—æ–‡å­—çš„ token æ•¸é‡
    Count(text string) int
    
    // ä¼°ç®—å›æ‡‰æ‰€éœ€çš„ token
    EstimateResponseTokens(prompt string) int
    
    // æˆªæ–·æ–‡å­—åˆ°æŒ‡å®š token æ•¸
    Truncate(text string, maxTokens int) string
}

// æ™ºæ…§ token å„ªåŒ–
type TokenOptimizer struct {
    maxContextTokens int
    reserveTokens    int  // ç‚ºå›æ‡‰ä¿ç•™çš„ token
    
    // å„ªåŒ–ç­–ç•¥
    strategies []OptimizationStrategy
}

// å„ªåŒ–ç­–ç•¥ç¯„ä¾‹
func (o *TokenOptimizer) Optimize(messages []Message) []Message {
    // 1. ç§»é™¤å†—é¤˜å…§å®¹
    messages = o.removeRedundancy(messages)
    
    // 2. æ‘˜è¦é•·å°è©±
    messages = o.summarizeOldMessages(messages)
    
    // 3. å£“ç¸®ç³»çµ±æç¤ºè©
    messages = o.compressSystemPrompt(messages)
    
    return messages
}
```

### 3. éŒ¯èª¤è™•ç†èˆ‡æ¢å¾© (Error Handling & Recovery)

```go
// AI éŒ¯èª¤é¡å‹
type AIError struct {
    Type        ErrorType
    Provider    Provider
    StatusCode  int
    Message     string
    Retryable   bool
    RetryAfter  time.Duration
}

// éŒ¯èª¤æ¢å¾©ç­–ç•¥
type RecoveryStrategy struct {
    // é‡è©¦ç­–ç•¥
    MaxRetries     int
    BackoffFactor  float64
    
    // é™ç´šç­–ç•¥
    FallbackProvider Provider
    SimplifyRequest  bool
    
    // æ–·è·¯å™¨
    CircuitBreaker *CircuitBreaker
}

// æ™ºæ…§éŒ¯èª¤è™•ç†
func (s *Service) HandleError(err error, strategy RecoveryStrategy) error {
    switch e := err.(type) {
    case *RateLimitError:
        // ç­‰å¾…å¾Œé‡è©¦
        time.Sleep(e.RetryAfter)
        return s.retryWithBackoff(strategy)
        
    case *ContextLengthError:
        // å„ªåŒ–ä¸Šä¸‹æ–‡å¾Œé‡è©¦
        return s.retryWithOptimizedContext()
        
    case *ProviderError:
        // åˆ‡æ›åˆ°å‚™ç”¨æä¾›è€…
        return s.fallbackToAlternative(strategy.FallbackProvider)
        
    default:
        return fmt.Errorf("unrecoverable AI error: %w", err)
    }
}
```

## ğŸ” ä½¿ç”¨ç¯„ä¾‹

### åŸºæœ¬èŠå¤©
```go
// å‰µå»º AI æœå‹™
aiService := ai.NewService(
    ai.WithProvider(ai.ProviderClaude),
    ai.WithAPIKey(apiKey),
    ai.WithModel("claude-3-opus-20240229"),
)

// ç™¼é€èŠå¤©è«‹æ±‚
response, err := aiService.Chat(ctx, ai.ChatRequest{
    Messages: []ai.Message{
        {
            Role:    ai.RoleUser,
            Content: "è§£é‡‹ Go çš„ interface æ¦‚å¿µ",
        },
    },
    MaxTokens:   1000,
    Temperature: 0.7,
})
```

### ä¸²æµå›æ‡‰
```go
// å‰µå»ºä¸²æµè«‹æ±‚
stream, err := aiService.ChatStream(ctx, ai.ChatRequest{
    Messages: messages,
    Options: ai.StreamOptions{
        ChunkSize:      100,
        IncludeUsage:   true,
        StopOnError:    false,
    },
})

// è™•ç†ä¸²æµäº‹ä»¶
for event := range stream {
    switch event.Type {
    case ai.StreamEventTypeContent:
        fmt.Print(event.Content)
    case ai.StreamEventTypeError:
        log.Error("ä¸²æµéŒ¯èª¤", "error", event.Error)
    case ai.StreamEventTypeEnd:
        fmt.Printf("\nä½¿ç”¨ tokens: %d\n", event.Metadata["usage"])
    }
}
```

### åµŒå…¥å‘é‡ç”Ÿæˆ
```go
// å‰µå»ºåµŒå…¥æœå‹™
embedService := ai.NewEmbeddingService(
    ai.WithEmbeddingModel("text-embedding-ada-002"),
    ai.WithBatchSize(100),
)

// ç”Ÿæˆå‘é‡
embeddings, err := embedService.GenerateEmbeddings(ctx, []string{
    "Go æ˜¯ä¸€å€‹éœæ…‹é¡å‹çš„ç·¨è­¯èªè¨€",
    "Python æ˜¯ä¸€å€‹å‹•æ…‹é¡å‹çš„è§£é‡‹èªè¨€",
    "Rust æ³¨é‡è¨˜æ†¶é«”å®‰å…¨",
})

// è¨ˆç®—ç›¸ä¼¼åº¦
similarity := embedService.CosineSimilarity(embeddings[0], embeddings[1])
fmt.Printf("Go èˆ‡ Python çš„ç›¸ä¼¼åº¦: %.2f\n", similarity)
```

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦
```go
func TestAIService_Chat(t *testing.T) {
    // ä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯
    mockClient := NewMockAIClient()
    service := ai.NewService(ai.WithClient(mockClient))
    
    // æ¸¬è©¦æ­£å¸¸æƒ…æ³
    t.Run("successful chat", func(t *testing.T) {
        mockClient.SetResponse(&ai.ChatResponse{
            Content: "æ¸¬è©¦å›æ‡‰",
            Usage:   ai.Usage{TotalTokens: 100},
        })
        
        response, err := service.Chat(ctx, testRequest)
        require.NoError(t, err)
        assert.Equal(t, "æ¸¬è©¦å›æ‡‰", response.Content)
    })
    
    // æ¸¬è©¦éŒ¯èª¤è™•ç†
    t.Run("rate limit error", func(t *testing.T) {
        mockClient.SetError(&ai.RateLimitError{
            RetryAfter: 60 * time.Second,
        })
        
        _, err := service.Chat(ctx, testRequest)
        assert.ErrorIs(t, err, ai.ErrRateLimit)
    })
}
```

## ğŸ”§ é…ç½®é¸é …

```yaml
ai:
  # æä¾›è€…é…ç½®
  providers:
    claude:
      api_key: ${CLAUDE_API_KEY}
      model: claude-3-opus-20240229
      max_tokens: 4096
      temperature: 0.7
      
    gemini:
      api_key: ${GEMINI_API_KEY}
      model: gemini-pro
      max_tokens: 8192
      temperature: 0.8
  
  # è·¯ç”±é…ç½®
  routing:
    primary_provider: claude
    fallback_provider: gemini
    health_check_interval: 30s
    
  # ä¸²æµé…ç½®
  streaming:
    buffer_size: 4096
    chunk_timeout: 100ms
    enable_compression: true
    
  # Token ç®¡ç†
  tokens:
    max_context_tokens: 100000
    reserve_response_tokens: 2000
    enable_optimization: true
    
  # åµŒå…¥å‘é‡
  embeddings:
    provider: openai
    model: text-embedding-ada-002
    cache_size: 10000
    batch_size: 100
```

## ğŸ“ˆ æ•ˆèƒ½å„ªåŒ–

1. **å¿«å–ç­–ç•¥**
   - LRU å¿«å–é »ç¹è«‹æ±‚
   - åµŒå…¥å‘é‡å¿«å–
   - æç¤ºè©æ¨¡æ¿å¿«å–

2. **æ‰¹æ¬¡è™•ç†**
   - æ‰¹é‡åµŒå…¥å‘é‡ç”Ÿæˆ
   - è«‹æ±‚åˆä½µå„ªåŒ–
   - éåŒæ­¥è™•ç†ä½‡åˆ—

3. **è³‡æºç®¡ç†**
   - é€£ç·šæ± ç®¡ç†
   - Token é ç®—æ§åˆ¶
   - è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–

## ğŸš€ æœªä¾†è¦åŠƒ

1. **æ›´å¤š AI æä¾›è€…**
   - OpenAI GPT-4 æ•´åˆ
   - Llama æœ¬åœ°æ¨¡å‹æ”¯æ´
   - å°ˆæ¥­é ˜åŸŸæ¨¡å‹

2. **é€²éšåŠŸèƒ½**
   - å¤šæ¨¡æ…‹æ”¯æ´ï¼ˆåœ–ç‰‡ã€éŸ³è¨Šï¼‰
   - å‡½æ•¸èª¿ç”¨ï¼ˆFunction Callingï¼‰
   - æ™ºæ…§å°è©±ç®¡ç†

3. **å„ªåŒ–å¢å¼·**
   - åˆ†æ•£å¼æ¨ç†
   - é‚Šç·£éƒ¨ç½²æ”¯æ´
   - è‡ªé©æ‡‰æ¨¡å‹é¸æ“‡

## ğŸ“š ç›¸é—œæ–‡ä»¶

- [Assistant Package README](../assistant/README-zh-TW.md) - åŠ©ç†æ ¸å¿ƒå¥—ä»¶
- [Embeddings å­å¥—ä»¶](./embeddings/README.md) - åµŒå…¥å‘é‡è©³ç´°èªªæ˜
- [Prompts å­å¥—ä»¶](./prompts/README.md) - æç¤ºè©å·¥ç¨‹æŒ‡å—
- [ä¸»è¦æ¶æ§‹æ–‡ä»¶](../../CLAUDE-ARCHITECTURE.md) - ç³»çµ±æ¶æ§‹æŒ‡å—