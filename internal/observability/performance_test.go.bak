package observability

import (
	"log/slog"
	"testing"
	"time"

	"github.com/koopa0/assistant-go/test/testutil"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestRateLimiter(t *testing.T) {
	logger := testutil.CreateTestLogger(slog.LevelDebug)
	collector := NewMetricsCollector(logger)
	rateLimiter := NewRateLimiter(collector, logger)

	t.Run("no_limit_set", func(t *testing.T) {
		// Should allow requests when no limit is set
		allowed := rateLimiter.CheckLimit("test-provider", 100)
		assert.True(t, allowed, "Should allow request when no limit set")
	})

	t.Run("within_limits", func(t *testing.T) {
		// Set limits
		rateLimiter.SetProviderLimit("test-provider", 10, 1000)

		// Should allow requests within limits
		for i := 0; i < 5; i++ {
			allowed := rateLimiter.CheckLimit("test-provider", 50)
			assert.True(t, allowed, "Should allow request within limits")
		}
	})

	t.Run("exceed_request_limit", func(t *testing.T) {
		rateLimiter.SetProviderLimit("request-limit-test", 3, 1000)

		// Use up the request limit
		for i := 0; i < 3; i++ {
			allowed := rateLimiter.CheckLimit("request-limit-test", 10)
			assert.True(t, allowed, "Should allow request within limit")
		}

		// Next request should be blocked
		allowed := rateLimiter.CheckLimit("request-limit-test", 10)
		assert.False(t, allowed, "Should block request exceeding limit")

		// Check that rate limit hit was recorded
		metric := collector.GetMetric("rate_limit_hits_total", map[string]string{"provider": "request-limit-test"})
		assert.NotNil(t, metric, "Should record rate limit hit")
		assert.Equal(t, 1.0, metric.Value, "Should record one rate limit hit")
	})

	t.Run("exceed_token_limit", func(t *testing.T) {
		rateLimiter.SetProviderLimit("token-limit-test", 10, 100)

		// Use up most of the token limit
		allowed := rateLimiter.CheckLimit("token-limit-test", 80)
		assert.True(t, allowed, "Should allow request within token limit")

		// Next request should exceed token limit
		allowed = rateLimiter.CheckLimit("token-limit-test", 30)
		assert.False(t, allowed, "Should block request exceeding token limit")
	})

	t.Run("limit_reset_after_minute", func(t *testing.T) {
		rateLimiter.SetProviderLimit("reset-test", 1, 100)

		// Use up the limit
		allowed := rateLimiter.CheckLimit("reset-test", 50)
		assert.True(t, allowed, "Should allow first request")

		allowed = rateLimiter.CheckLimit("reset-test", 50)
		assert.False(t, allowed, "Should block second request")

		// Manually reset the last reset time to simulate time passage
		status := rateLimiter.GetLimitStatus()
		require.Contains(t, status, "reset-test")

		// In a real scenario, we'd wait a minute, but for testing we manipulate the state
		rateLimiter.limits["reset-test"].LastReset = time.Now().Add(-2 * time.Minute)

		// Should allow request after reset
		allowed = rateLimiter.CheckLimit("reset-test", 50)
		assert.True(t, allowed, "Should allow request after reset")
	})

	t.Run("get_limit_status", func(t *testing.T) {
		rateLimiter.SetProviderLimit("status-test", 5, 500)

		// Make some requests
		rateLimiter.CheckLimit("status-test", 100)
		rateLimiter.CheckLimit("status-test", 150)

		status := rateLimiter.GetLimitStatus()
		require.Contains(t, status, "status-test")

		providerStatus := status["status-test"]
		assert.Equal(t, 5, providerStatus.RequestsPerMinute)
		assert.Equal(t, 500, providerStatus.TokensPerMinute)
		assert.Equal(t, 2, providerStatus.RequestCount)
		assert.Equal(t, 250, providerStatus.TokenCount)
		assert.False(t, providerStatus.Blocked)
	})
}

func TestPerformanceRegression(t *testing.T) {
	logger := testutil.CreateTestLogger(slog.LevelDebug)
	collector := NewMetricsCollector(logger)
	regression := NewPerformanceRegression(collector, logger)

	t.Run("no_baseline_set", func(t *testing.T) {
		// Should not detect regression when no baseline is set
		hasRegression := regression.CheckRegression("test-metric", 100.0)
		assert.False(t, hasRegression, "Should not detect regression without baseline")
	})

	t.Run("within_threshold", func(t *testing.T) {
		// Set baseline with 20% threshold
		regression.SetBaseline("response-time", 100.0, 20.0)

		// Test value within threshold (10% increase)
		hasRegression := regression.CheckRegression("response-time", 110.0)
		assert.False(t, hasRegression, "Should not detect regression within threshold")
	})

	t.Run("exceeds_threshold", func(t *testing.T) {
		// Set baseline with 20% threshold
		regression.SetBaseline("error-rate", 1.0, 50.0)

		// Test value exceeding threshold (100% increase)
		hasRegression := regression.CheckRegression("error-rate", 2.0)
		assert.True(t, hasRegression, "Should detect regression exceeding threshold")

		// Check that regression was recorded
		metric := collector.GetMetric("performance_regressions_total", map[string]string{"metric": "error-rate"})
		assert.NotNil(t, metric, "Should record performance regression")
		assert.Equal(t, 1.0, metric.Value, "Should record one regression")

		// Check regression percentage
		percentMetric := collector.GetMetric("performance_regression_percent", map[string]string{"metric": "error-rate"})
		assert.NotNil(t, percentMetric, "Should record regression percentage")
		assert.Equal(t, 100.0, percentMetric.Value, "Should record 100% regression")
	})

	t.Run("improvement_not_regression", func(t *testing.T) {
		// Set baseline
		regression.SetBaseline("latency", 200.0, 25.0)

		// Test improved value (50% decrease)
		hasRegression := regression.CheckRegression("latency", 100.0)
		assert.False(t, hasRegression, "Should not detect regression for improvement")
	})

	t.Run("multiple_metrics", func(t *testing.T) {
		// Set baselines for multiple metrics
		regression.SetBaseline("cpu-usage", 50.0, 30.0)
		regression.SetBaseline("memory-usage", 1000.0, 25.0)

		// Test one regression, one within threshold
		cpuRegression := regression.CheckRegression("cpu-usage", 70.0)         // 40% increase
		memoryRegression := regression.CheckRegression("memory-usage", 1200.0) // 20% increase

		assert.True(t, cpuRegression, "Should detect CPU regression")
		assert.False(t, memoryRegression, "Should not detect memory regression")
	})
}

func TestAIMetricsIntegration(t *testing.T) {
	logger := testutil.CreateTestLogger(slog.LevelDebug)
	collector := NewMetricsCollector(logger)
	aiMetrics := NewAIMetrics(collector, logger)

	t.Run("record_successful_request", func(t *testing.T) {
		startTime := time.Now().Add(-100 * time.Millisecond)
		tokenUsage := map[string]int{
			"input":  50,
			"output": 30,
			"total":  80,
		}

		aiMetrics.RecordRequest("claude", "claude-3-haiku", startTime, true, tokenUsage, "")

		// Check that metrics were recorded
		requestMetric := collector.GetMetric("ai_requests_total", map[string]string{
			"provider": "claude",
			"model":    "claude-3-haiku",
		})
		assert.NotNil(t, requestMetric, "Should record request metric")
		assert.Equal(t, 1.0, requestMetric.Value, "Should record one request")

		successMetric := collector.GetMetric("ai_requests_success_total", map[string]string{
			"provider": "claude",
			"model":    "claude-3-haiku",
		})
		assert.NotNil(t, successMetric, "Should record success metric")
		assert.Equal(t, 1.0, successMetric.Value, "Should record one success")

		tokenMetric := collector.GetMetric("ai_tokens_total", map[string]string{
			"provider": "claude",
			"model":    "claude-3-haiku",
		})
		assert.NotNil(t, tokenMetric, "Should record token metric")
		assert.Equal(t, 80.0, tokenMetric.Value, "Should record total tokens")
	})

	t.Run("record_failed_request", func(t *testing.T) {
		startTime := time.Now().Add(-200 * time.Millisecond)

		aiMetrics.RecordRequest("gemini", "gemini-pro", startTime, false, nil, "rate_limit")

		// Check error metric
		errorMetric := collector.GetMetric("ai_requests_error_total", map[string]string{
			"provider":   "gemini",
			"model":      "gemini-pro",
			"error_type": "rate_limit",
		})
		assert.NotNil(t, errorMetric, "Should record error metric")
		assert.Equal(t, 1.0, errorMetric.Value, "Should record one error")
	})

	t.Run("record_embedding_request", func(t *testing.T) {
		startTime := time.Now().Add(-50 * time.Millisecond)

		aiMetrics.RecordEmbedding("openai", "text-embedding-ada-002", startTime, true, 25, 1536, "")

		// Check embedding metrics
		embeddingMetric := collector.GetMetric("ai_embeddings_total", map[string]string{
			"provider": "openai",
			"model":    "text-embedding-ada-002",
		})
		assert.NotNil(t, embeddingMetric, "Should record embedding metric")
		assert.Equal(t, 1.0, embeddingMetric.Value, "Should record one embedding")

		dimensionMetric := collector.GetMetric("ai_embedding_dimensions", map[string]string{
			"provider": "openai",
			"model":    "text-embedding-ada-002",
		})
		assert.NotNil(t, dimensionMetric, "Should record dimension metric")
		assert.Equal(t, 1536.0, dimensionMetric.Value, "Should record embedding dimensions")
	})

	t.Run("get_ai_stats", func(t *testing.T) {
		// Record some requests first
		startTime := time.Now().Add(-100 * time.Millisecond)
		tokenUsage := map[string]int{"total": 100}

		aiMetrics.RecordRequest("claude", "claude-3-haiku", startTime, true, tokenUsage, "")
		aiMetrics.RecordRequest("claude", "claude-3-haiku", startTime, true, tokenUsage, "")
		aiMetrics.RecordRequest("claude", "claude-3-haiku", startTime, false, nil, "timeout")

		stats := aiMetrics.GetAIStats()
		assert.Contains(t, stats, "providers", "Stats should contain providers")
		assert.Contains(t, stats, "last_updated", "Stats should contain last_updated")

		providers := stats["providers"].(map[string]map[string]interface{})
		assert.Contains(t, providers, "claude", "Should have claude provider stats")

		claudeStats := providers["claude"]
		assert.Equal(t, 3.0, claudeStats["total_requests"], "Should have 3 total requests")
		assert.Equal(t, 2.0, claudeStats["successful_requests"], "Should have 2 successful requests")
		assert.Equal(t, 1.0, claudeStats["failed_requests"], "Should have 1 failed request")
		assert.Equal(t, 200.0, claudeStats["total_tokens"], "Should have 200 total tokens")
	})
}

func TestPerformanceMonitorIntegration(t *testing.T) {
	logger := testutil.CreateTestLogger(slog.LevelDebug)
	monitor := NewPerformanceMonitor(logger)

	t.Run("get_system_stats", func(t *testing.T) {
		// Record some AI metrics
		aiMetrics := monitor.GetAIMetrics()
		startTime := time.Now().Add(-100 * time.Millisecond)
		tokenUsage := map[string]int{"total": 50}
		aiMetrics.RecordRequest("test-provider", "test-model", startTime, true, tokenUsage, "")

		stats := monitor.GetSystemStats()
		assert.Contains(t, stats, "uptime_seconds", "Should contain uptime")
		assert.Contains(t, stats, "ai", "Should contain AI stats")
		assert.Contains(t, stats, "metrics_count", "Should contain metrics count")

		uptime := stats["uptime_seconds"].(float64)
		assert.Greater(t, uptime, 0.0, "Uptime should be positive")

		metricsCount := stats["metrics_count"].(int)
		assert.Greater(t, metricsCount, 0, "Should have some metrics")
	})

	t.Run("health_check", func(t *testing.T) {
		err := monitor.Health(nil)
		assert.NoError(t, err, "Health check should pass")

		// Check that health metric was recorded
		healthMetric := monitor.GetCollector().GetMetric("system_health", map[string]string{
			"component": "performance_monitor",
		})
		assert.NotNil(t, healthMetric, "Should record health metric")
		assert.Equal(t, 1.0, healthMetric.Value, "Health should be 1")
	})
}

// BenchmarkMetricsCollection benchmarks metrics collection performance
func BenchmarkMetricsCollection(b *testing.B) {
	logger := testutil.CreateTestLogger(slog.LevelError)
	collector := NewMetricsCollector(logger)

	labels := map[string]string{
		"provider": "test",
		"model":    "test-model",
	}

	b.Run("counter", func(b *testing.B) {
		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			collector.Counter("test_counter", labels, 1.0)
		}
	})

	b.Run("gauge", func(b *testing.B) {
		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			collector.Gauge("test_gauge", labels, float64(i))
		}
	})

	b.Run("histogram", func(b *testing.B) {
		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			collector.Histogram("test_histogram", labels, float64(i))
		}
	})
}

// BenchmarkRateLimiter benchmarks rate limiter performance
func BenchmarkRateLimiter(b *testing.B) {
	logger := testutil.CreateTestLogger(slog.LevelError)
	collector := NewMetricsCollector(logger)
	rateLimiter := NewRateLimiter(collector, logger)

	rateLimiter.SetProviderLimit("test-provider", 1000000, 1000000) // High limits

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		rateLimiter.CheckLimit("test-provider", 10)
	}
}
